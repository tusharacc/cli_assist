# Hugging Face Dependencies for Enterprise LLM Replica (GPT-4 Simulation)
# Install with: pip install -r requirements_huggingface.txt

# Core Hugging Face libraries
transformers>=4.30.0
torch>=2.0.0
tokenizers>=0.13.0
accelerate>=0.20.0

# GPT-4 Simulation Models (Large Language Models)
# microsoft/DialoGPT-large - Good for conversational AI
# microsoft/DialoGPT-medium - Smaller alternative
# microsoft/DialoGPT-small - Lightweight option

# Alternative large models for GPT-4 simulation:
# EleutherAI/gpt-neo-2.7B - Large generative model
# EleutherAI/gpt-j-6B - Even larger model
# microsoft/DialoGPT-large - Conversational AI
# facebook/opt-1.3b - Open Pre-trained Transformer

# Model-specific dependencies
datasets>=2.12.0
evaluate>=0.4.0
rouge-score>=0.1.2
sacrebleu>=2.3.0

# Additional utilities
numpy>=1.24.0
pandas>=2.0.0
scikit-learn>=1.3.0
tqdm>=4.65.0

# Memory optimization
psutil>=5.9.0
GPUtil>=1.4.0

# Optional: For better performance
# torch-audio>=2.0.0  # Uncomment if using audio models
# torch-vision>=0.15.0  # Uncomment if using vision models

# Development and testing
pytest>=7.4.0
pytest-cov>=4.1.0
black>=23.0.0
flake8>=6.0.0

# Note: Install PyTorch with CUDA support if you have a GPU:
# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Note: For GPT-4 simulation, consider these models:
# 1. microsoft/DialoGPT-large (2.7B parameters) - Good balance
# 2. EleutherAI/gpt-neo-2.7B (2.7B parameters) - More general purpose
# 3. EleutherAI/gpt-j-6B (6B parameters) - Larger, more capable
# 4. facebook/opt-1.3b (1.3B parameters) - Smaller, faster
