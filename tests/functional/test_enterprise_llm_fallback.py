#!/usr/bin/env python3
"""
Test script for Enterprise LLM Replica with Fallback Models
Tests the OAuth2 framework with OpenAI GPT-4 and Hugging Face fallbacks
"""

import sys
import os
sys.path.append('src')

def test_enterprise_llm_fallback():
    """Test the Enterprise LLM Replica with fallback models"""
    print("üß™ Testing Enterprise LLM Replica with Fallback Models...")
    
    try:
        from lumos_cli.enterprise_llm_replica import get_enterprise_llm_replica
        
        # Initialize the Enterprise LLM Replica
        print("\n1. Initializing Enterprise LLM Replica...")
        enterprise_llm = get_enterprise_llm_replica()
        
        # Show configuration status
        print("\n2. Checking configuration status...")
        model_info = enterprise_llm.get_model_info()
        print(f"   Configured: {model_info['configured']}")
        print(f"   Has Token: {model_info['has_token']}")
        print(f"   Token URL: {model_info['token_url'] or 'Not set'}")
        print(f"   Chat URL: {model_info['chat_url'] or 'Not set'}")
        
        # Test connection (will use fallback models)
        print("\n3. Testing connection (using fallback models)...")
        if enterprise_llm.test_connection():
            print("   ‚úÖ Enterprise LLM Replica connection successful")
        else:
            print("   ‚ùå Enterprise LLM Replica connection failed")
            return False
        
        # Test generate_response
        print("\n4. Testing generate_response...")
        response = enterprise_llm.generate_response(
            "Explain what microservices architecture is in 2 sentences",
            max_tokens=100,
            temperature=0.7
        )
        print(f"   Response: {response[:200]}...")
        
        # Test chat interface
        print("\n5. Testing chat interface...")
        messages = [
            {"role": "system", "content": "You are a helpful AI assistant"},
            {"role": "user", "content": "What are the benefits of using containers?"}
        ]
        chat_response = enterprise_llm.chat(messages)
        print(f"   Chat Response: {chat_response[:200]}...")
        
        # Test code generation
        print("\n6. Testing code generation...")
        code_response = enterprise_llm.generate_code(
            "create a simple Python function to calculate fibonacci numbers",
            language="python"
        )
        print(f"   Code Response: {code_response[:200]}...")
        
        print("\n‚úÖ Enterprise LLM Replica with fallback models test completed successfully!")
        return True
        
    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_enterprise_configuration():
    """Test Enterprise LLM configuration without real credentials"""
    print("\nüè¢ Testing Enterprise LLM Configuration...")
    
    try:
        from src.lumos_cli.config.enterprise_llm_config import get_enterprise_llm_config_manager
        
        # Initialize configuration manager
        print("\n1. Initializing configuration manager...")
        config_manager = get_enterprise_llm_config_manager()
        
        # Show current configuration
        print("\n2. Current configuration status...")
        config_manager.show_config()
        
        # Test configuration without real credentials
        print("\n3. Testing configuration with mock credentials...")
        from lumos_cli.enterprise_llm_replica import get_enterprise_llm_replica
        
        enterprise_llm = get_enterprise_llm_replica()
        
        # Configure with mock credentials (won't work but tests the framework)
        enterprise_llm.configure(
            token_url="https://mock-enterprise.com/oauth2/token",
            chat_url="https://mock-enterprise.com/api/v1/chat/completions",
            app_id="mock-client-id",
            app_key="mock-client-secret",
            app_resource="https://mock-enterprise.com/api"
        )
        
        print("   ‚úÖ Mock configuration set successfully")
        
        # Test that it falls back to local models
        print("\n4. Testing fallback to local models...")
        response = enterprise_llm.generate_response(
            "This should use fallback models since mock credentials won't work",
            max_tokens=50
        )
        print(f"   Fallback Response: {response[:200]}...")
        
        print("\n‚úÖ Enterprise LLM configuration test completed successfully!")
        return True
        
    except Exception as e:
        print(f"‚ùå Configuration test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_model_fallback_priority():
    """Test the priority of fallback models"""
    print("\nüîÑ Testing Model Fallback Priority...")
    
    try:
        from lumos_cli.enterprise_llm_replica import get_enterprise_llm_replica
        
        enterprise_llm = get_enterprise_llm_replica()
        
        print("\n1. Testing OpenAI GPT-4 availability...")
        if enterprise_llm._try_openai_gpt4("", None, None):
            print("   ‚úÖ OpenAI GPT-4 is available")
            print("   üìù Will use OpenAI GPT-4 as primary fallback")
        else:
            print("   ‚ùå OpenAI GPT-4 is not available")
            print("   üìù Will use Hugging Face models as fallback")
        
        print("\n2. Testing Hugging Face GPT-4 simulation...")
        try:
            from lumos_cli.gpt4_simulator import get_gpt4_simulator
            gpt4_simulator = get_gpt4_simulator()
            print("   ‚úÖ Hugging Face GPT-4 simulator is available")
        except Exception as e:
            print(f"   ‚ùå Hugging Face GPT-4 simulator not available: {e}")
        
        print("\n3. Testing actual fallback behavior...")
        response = enterprise_llm.generate_response(
            "Test the fallback model priority",
            max_tokens=30
        )
        print(f"   Fallback Response: {response[:100]}...")
        
        print("\n‚úÖ Model fallback priority test completed!")
        return True
        
    except Exception as e:
        print(f"‚ùå Fallback priority test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Main test function"""
    print("üè¢ Enterprise LLM Replica with Fallback Models Test Suite")
    print("=" * 70)
    
    # Test Enterprise LLM with fallback models
    fallback_success = test_enterprise_llm_fallback()
    
    # Test Enterprise configuration
    config_success = test_enterprise_configuration()
    
    # Test model fallback priority
    priority_success = test_model_fallback_priority()
    
    # Summary
    print("\n" + "=" * 70)
    print("üìä Test Results Summary:")
    print("=" * 70)
    
    print(f"Enterprise LLM Fallback: {'‚úÖ PASSED' if fallback_success else '‚ùå FAILED'}")
    print(f"Enterprise Configuration: {'‚úÖ PASSED' if config_success else '‚ùå FAILED'}")
    print(f"Model Fallback Priority: {'‚úÖ PASSED' if priority_success else '‚ùå FAILED'}")
    
    if fallback_success and config_success and priority_success:
        print("\nüéâ All tests passed! Enterprise LLM Replica with fallback models is ready!")
        print("\nüìã What this means:")
        print("   ‚Ä¢ OAuth2 framework is ready for real enterprise credentials")
        print("   ‚Ä¢ Fallback to OpenAI GPT-4 when available")
        print("   ‚Ä¢ Fallback to Hugging Face models when OpenAI not available")
        print("   ‚Ä¢ Seamless testing without real enterprise credentials")
        print("   ‚Ä¢ Production-ready when real credentials are provided")
    else:
        print("\n‚ö†Ô∏è  Some tests failed. Please check the errors above.")
    
    print("\nüöÄ Next steps:")
    print("1. Configure with real enterprise credentials when available")
    print("2. Test with actual enterprise API")
    print("3. Integrate with Lumos CLI")
    print("4. Deploy to production environment")

if __name__ == "__main__":
    main()
